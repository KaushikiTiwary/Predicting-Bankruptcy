# -*- coding: utf-8 -*-
"""Final_Fintech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/KaushikiTiwary/Predicting-Bankruptcy/blob/main/Final_Fintech.ipynb

#Non Bankrupt Data
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

non_bankrupt_df = pd.read_csv('/content/zvei35wzg5ry6rid.csv')  # change to your file name
non_bankrupt_df.head()

columns_to_keep = [
    'tic', 'fyear', 'act', 'lct', 'at', 'seq',
    'ebit', 'sale', 'lt', 'prcc_f', 'csho', 'sic'
]

non_bankrupt_df = non_bankrupt_df[columns_to_keep]

print(non_bankrupt_df.columns)

non_bankrupt_df['sic'].unique()

sic_to_industry_custom = {
    # âœ… Healthcare
    8011: 'Healthcare', 8021: 'Healthcare', 8051: 'Healthcare', 8062: 'Healthcare',
    2834: 'Healthcare', 3841: 'Healthcare', 3826: 'Healthcare', 3829: 'Healthcare',
    2833: 'Healthcare', 3845: 'Healthcare',

    # âœ… Finance & Banking
    6020: 'Finance & Banking', 6021: 'Finance & Banking', 6035: 'Finance & Banking',
    6036: 'Finance & Banking', 6111: 'Finance & Banking', 6141: 'Finance & Banking',

    # âœ… Tech
    3571: 'Tech', 3674: 'Tech', 7370: 'Tech', 7371: 'Tech', 7372: 'Tech', 7373: 'Tech',
    7374: 'Tech', 3577: 'Tech', 3572: 'Tech', 5045: 'Tech', 5065: 'Tech',

    # âœ… Manufacturing
    3711: 'Manufacturing', 3721: 'Manufacturing', 3761: 'Manufacturing', 3841: 'Manufacturing',
    3441: 'Manufacturing', 3561: 'Manufacturing', 2833: 'Manufacturing',

    # âœ… Retail
    5411: 'Retail', 5331: 'Retail', 5311: 'Retail', 5999: 'Retail', 5621: 'Retail',

    # âœ… Energy, Oil & Gas
    1311: 'Energy, Oil & Gas', 1389: 'Energy, Oil & Gas', 2911: 'Energy, Oil & Gas',
    4922: 'Energy, Oil & Gas', 4932: 'Energy, Oil & Gas', 4931: 'Energy, Oil & Gas',

    # âœ… Misc/Other
    2836: 'Chemicals', 2040: 'Consumer Products', 2086: 'Consumer Products',
}

non_bankrupt_df['industry'] = non_bankrupt_df['sic'].map(sic_to_industry_custom)

non_bankrupt_df['industry'].value_counts(dropna=False)

non_bankrupt_df = non_bankrupt_df.dropna(subset=['industry'])

non_bankrupt_df.isnull().sum()

non_bankrupt_df = non_bankrupt_df.dropna()

non_bankrupt_df.isnull().sum()

non_bankrupt_df.groupby('industry')['tic'].nunique().sort_values(ascending=False)

healthcare_non_bankrupt = non_bankrupt_df[non_bankrupt_df['industry'] == 'Healthcare']

#healthcare non_bankrupt

# Calculate individual Z-Score components
healthcare_non_bankrupt['X1'] = (healthcare_non_bankrupt['act'] - healthcare_non_bankrupt['lct']) / healthcare_non_bankrupt['at'] # Working Capital / Total Assets
healthcare_non_bankrupt['X2'] = healthcare_non_bankrupt['seq'] / healthcare_non_bankrupt['at']                          # Retained Earnings / Total Assets
healthcare_non_bankrupt['X3'] = healthcare_non_bankrupt['ebit'] / healthcare_non_bankrupt['at']                         # EBIT / Total Assets
healthcare_non_bankrupt['X4'] = (healthcare_non_bankrupt['prcc_f'] * healthcare_non_bankrupt['csho']) / healthcare_non_bankrupt['lt'] # Market Value of Equity / Total Liabilities
healthcare_non_bankrupt['X5'] = healthcare_non_bankrupt['sale'] / healthcare_non_bankrupt['at']                         # Sales / Total Assets
healthcare_non_bankrupt.head()

healthcare_non_bankrupt

"""#Bankrupt Data"""

from google.colab import files
uploaded = files.upload()

!pip install wrds

import wrds

# Connect to WRDS (first time it'll ask for username/password)
db = wrds.Connection()

import pandas as pd

# Load tickers from each uploaded file
tech = pd.read_csv("Tech.txt", sep="\t")
retail = pd.read_csv("Retail.txt", sep="\t")
manufacturing = pd.read_csv("Manufacturing.txt", sep="\t")
finance = pd.read_csv("Fin_Banking.txt", sep="\t")
energy = pd.read_csv("Energy_Oil_Gas.txt", sep="\t")
healthcare = pd.read_csv("Healthcare.txt", sep="\t")

# Add industry column
tech['industry'] = 'Tech'
retail['industry'] = 'Retail'
manufacturing['industry'] = 'Manufacturing'
finance['industry'] = 'Finance & Banking'
energy['industry'] = 'Energy, Oil & Gas'
healthcare['industry'] = 'Healthcare'

# Combine all
bankrupt_companies = pd.concat([tech, retail, manufacturing, finance, energy, healthcare], ignore_index=True)

# Required Z-score variables
variables = ['tic', 'fyear', 'datadate', 'act', 'lct', 'at', 'seq', 'ebit', 'sale', 'lt', 'prcc_f', 'csho']

# Initialize empty dataframe
all_data = pd.DataFrame()

# Loop through each ticker
for _, row in bankrupt_companies.iterrows():
    ticker = row['tic']
    industry = row['industry']

    query = f"""
        SELECT {', '.join(variables)}
        FROM comp.funda
        WHERE tic = '{ticker}'
        AND indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C'
    """
    try:
        df = db.raw_sql(query)
        df['industry'] = industry
        all_data = pd.concat([all_data, df], ignore_index=True)
    except Exception as e:
        print(f"Error fetching {ticker}: {e}")

all_data.to_csv("industry_wise_bankrupt_financials.csv", index=False)

# prompt: read the above csv file

import pandas as pd
df = pd.read_csv('industry_wise_bankrupt_financials.csv')
df

#count of unique comapnies in industries
unique_companies_by_industry = df.groupby('industry')['tic'].nunique()
print(unique_companies_by_industry)

# Count null (NaN) values in each column
null_counts = df.isnull().sum()
# Show columns that have at least 1 null
null_counts[null_counts > 0].sort_values(ascending=False)

df.shape

# Check year range across dataset
print("Min year:", df['fyear'].min())
print("Max year:", df['fyear'].max())

# Or count how many years per ticker
df.groupby('tic')['fyear'].nunique().sort_values(ascending=False)

z_score_columns = ['act', 'lct', 'at', 'seq', 'ebit', 'sale', 'lt', 'prcc_f', 'csho']
df_clean = df.dropna(subset=z_score_columns)

# 1. Create a lookup of each company's final fiscal year
last_fyear = bankrupt_companies.set_index('tic')['last_fyear'].to_dict()

# 2. Filter out years after bankruptcy
df = df[df.apply(lambda row: row['fyear'] <= last_fyear.get(row['tic'], 9999), axis=1)]
df

print(bankrupt_companies.columns)

#healthcare
healthcare_df = df_clean[df_clean['industry'] == 'Healthcare']

# Calculate individual Z-Score components
healthcare_df['X1'] = (healthcare_df['act'] - healthcare_df['lct']) / healthcare_df['at'] # Working Capital / Total Assets
healthcare_df['X2'] = healthcare_df['seq'] / healthcare_df['at']                          # Retained Earnings / Total Assets
healthcare_df['X3'] = healthcare_df['ebit'] / healthcare_df['at']                         # EBIT / Total Assets
healthcare_df['X4'] = (healthcare_df['prcc_f'] * healthcare_df['csho']) / healthcare_df['lt'] # Market Value of Equity / Total Liabilities
healthcare_df['X5'] = healthcare_df['sale'] / healthcare_df['at']                         # Sales / Total Assets
healthcare_df.head()

healthcare_df

healthcare_non_bankrupt

healthcare_non_bankrupt['tic'].nunique()

healthcare_df['tic'].nunique()

"""#LDA"""

# Label data
healthcare_df['label'] = 1  # bankrupt
healthcare_non_bankrupt['label'] = 0  # non-bankrupt

# Combine both
healthcare_full_df = pd.concat([healthcare_df, healthcare_non_bankrupt], ignore_index=True)

import numpy as np

# STEP 1: Sample equal number of companies (e.g., 91)
n_companies = 91

bankrupt_companies = healthcare_df['tic'].unique()
non_bankrupt_companies = healthcare_non_bankrupt['tic'].unique()

# STEP 2: Randomly sample 91 companies from non-bankrupt set
np.random.seed(42)
selected_nonbankrupt_tics = np.random.choice(non_bankrupt_companies, size=n_companies, replace=False)

# STEP 3: Filter full data based on those companies
sampled_non_bankrupt_df = healthcare_non_bankrupt[healthcare_non_bankrupt['tic'].isin(selected_nonbankrupt_tics)]
sampled_bankrupt_df = healthcare_df.copy()  # all 91 tic already present

# STEP 4: Combine
balanced_company_df = pd.concat([sampled_bankrupt_df, sampled_non_bankrupt_df], ignore_index=True)

len(sampled_bankrupt_df['tic'].unique())

len(sampled_non_bankrupt_df['tic'].unique())

balanced_company_df.drop(columns=['sic', 'datadate'])

"""#Run the LDA Model"""

import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Step 1: Replace any unexpected values
X = balanced_company_df[['X1', 'X2', 'X3', 'X4', 'X5']].replace([np.inf, -np.inf], np.nan)
X = X.fillna(X.mean())
y = balanced_company_df['label'].astype(int)

# Step 2: Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Train
lda = LinearDiscriminantAnalysis()
lda.fit(X_scaled, y)

# Step 4: Predict
y_pred = lda.predict(X_scaled)

# Step 5: Evaluate
print("Confusion Matrix:\n", confusion_matrix(y, y_pred))
print("\nClassification Report:\n", classification_report(y, y_pred))
print("\nAccuracy:", accuracy_score(y, y_pred))

# Step 6: AUC Curve
y_proba = lda.predict_proba(X_scaled)[:, 1]
fpr, tpr, thresholds = roc_curve(y, y_proba)
plt.plot(fpr, tpr, label="LDA")
plt.plot([0, 1], [0, 1], linestyle='--', label='Random')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Balanced LDA Model")
plt.legend()
plt.show()

print("AUC Score:", roc_auc_score(y, y_proba))

"""#Try equal numbers of rows"""

sampled_bankrupt_df.drop(columns='datadate')

sampled_non_bankrupt_df.drop(columns='sic')

"""#Final Model

"""

# Step 1: Count how many rows each company has
bankrupt_counts = sampled_bankrupt_df['tic'].value_counts()
non_bankrupt_counts = sampled_non_bankrupt_df['tic'].value_counts()

# Step 2: Create a dictionary {rows: [companies_with_that_row_count]}
from collections import defaultdict

def invert_counts(counts):
    grouped = defaultdict(list)
    for tic, count in counts.items():
        grouped[count].append(tic)
    return grouped

grouped_bankrupt = invert_counts(bankrupt_counts)
grouped_non_bankrupt = invert_counts(non_bankrupt_counts)

# Step 3: Match row counts between both groups
matching_tics = []

for row_count in grouped_bankrupt:
    if row_count in grouped_non_bankrupt:
        # Take equal number of companies from both groups
        n = min(len(grouped_bankrupt[row_count]), len(grouped_non_bankrupt[row_count]))
        bankrupt_subset = grouped_bankrupt[row_count][:n]
        non_bankrupt_subset = grouped_non_bankrupt[row_count][:n]

        matching_tics.extend([(b, nb, row_count) for b, nb in zip(bankrupt_subset, non_bankrupt_subset)])

# Extract the data for final balanced dataset
bankrupt_final = sampled_bankrupt_df[sampled_bankrupt_df['tic'].isin([b for b, _, _ in matching_tics])]
non_bankrupt_final = sampled_non_bankrupt_df[sampled_non_bankrupt_df['tic'].isin([nb for _, nb, _ in matching_tics])]

# Combine them
balanced_company_df = pd.concat([bankrupt_final, non_bankrupt_final], ignore_index=True)

balanced_company_df = balanced_company_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Replace inf and NaNs (very important)
X = balanced_company_df[['X1', 'X2', 'X3', 'X4', 'X5']]
y = balanced_company_df['label'].astype(int)

X.replace([np.inf, -np.inf], np.nan, inplace=True)
X.fillna(X.mean(), inplace=True)

from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

lda = LinearDiscriminantAnalysis()
lda.fit(X_scaled, y)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

y_pred = lda.predict(X_scaled)
y_proba = lda.predict_proba(X_scaled)[:, 1]

print(confusion_matrix(y, y_pred))
print(classification_report(y, y_pred))
print("Accuracy:", accuracy_score(y, y_pred))
print("AUC Score:", roc_auc_score(y, y_proba))

# ROC Curve
fpr, tpr, _ = roc_curve(y, y_proba)
plt.plot(fpr, tpr, label='LDA')
plt.plot([0,1], [0,1], linestyle='--', label='Random')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Balanced Company Data")
plt.legend()
plt.show()

lda_weights = pd.Series(lda.coef_[0], index=X.columns)
print("ðŸ“Š Altman-style LDA Z-Score Weights:")
print(lda_weights.sort_values(ascending=False))

# Step 1: Get the probabilities for the balanced dataset
balanced_company_df['lda_probability'] = lda.predict_proba(X_scaled)[:, 1]

# Step 2: If needed, merge probabilities back to the original 'df'
# This step depends on your needs and how 'balanced_company_df' relates to 'df'
# One way could be using 'merge'
# df = pd.merge(df, balanced_company_df[['tic', 'fyear', 'lda_probability']], on=['tic', 'fyear'], how='left')

# Step 3: Now calculate percentiles on balanced_company_df
q25 = balanced_company_df['lda_probability'].quantile(0.25)
q50 = balanced_company_df['lda_probability'].quantile(0.50)
q75 = balanced_company_df['lda_probability'].quantile(0.75)

print("Thresholds based on data:")
print(f"ðŸ”´ Very High Risk: < {q25:.2f}")
print(f"ðŸŸ§ High Risk: {q25:.2f} â€“ {q50:.2f}")
print(f"ðŸŸ¨ Medium Risk: {q50:.2f} â€“ {q75:.2f}")
print(f"ðŸŸ© Very Low Risk: > {q75:.2f}")

from sklearn.metrics import roc_curve

# Get predicted probabilities
y_proba = lda.predict_proba(X_scaled)[:, 1]

# Compute FPR, TPR, Thresholds
fpr, tpr, thresholds = roc_curve(y, y_proba)

# Find the threshold where (TPR - FPR) is maximized (Youden's J)
j_scores = tpr - fpr
best_index = j_scores.argmax()
best_threshold = thresholds[best_index]

print(f"ðŸ“Œ Best Threshold (Youdenâ€™s J): {best_threshold:.4f}")

import matplotlib.pyplot as plt

plt.plot(thresholds, tpr - fpr)
plt.xlabel("Threshold")
plt.ylabel("Youden's J (TPR - FPR)")
plt.title("Optimal Threshold for Bankruptcy Detection")
plt.grid(True)
plt.show()

# # Use Youden threshold for strict flag
# df['is_bankrupt_pred'] = (df['lda_probability'] < 0.5207).astype(int)

# # Use quantiles for colorful dashboards
# df['risk_zone'] = df['lda_probability'].apply(risk_zone_function)

"""#Companies check"""

# Get all unique companies used in sampled_non_bankrupt_df
used_companies = sampled_non_bankrupt_df['tic'].unique()

# Filter a company from healthcare_non_bankrupt not used in sampled set
unseen_company_df = healthcare_non_bankrupt[~healthcare_non_bankrupt['tic'].isin(used_companies)]

# Pick one company for demo
new_company = unseen_company_df[unseen_company_df['tic'] == unseen_company_df['tic'].unique()[0]]

# Select relevant features
features = ['X1', 'X2', 'X3', 'X4', 'X5']
new_X = new_company[features]

# Handle any NaNs or Infs (just like training data)
new_X.replace([np.inf, -np.inf], np.nan, inplace=True)
new_X.fillna(X.mean(), inplace=True)  # Use mean from original training X

# Use the scaler fitted on training data
new_X_scaled = scaler.transform(new_X)

# Predict probability (Z-score)
new_company['lda_probability'] = lda.predict_proba(new_X_scaled)[:, 1]

def risk_zone_function(prob):
    if prob > 0.54:
        return "ðŸŸ© Very Low Risk"
    elif prob > 0.51:
        return "ðŸŸ¨ Medium Risk"
    elif prob > 0.48:
        return "ðŸŸ§ High Risk"
    else:
        return "ðŸ”´ Very High Risk"

# Using your already defined function
new_company['risk_zone'] = new_company['lda_probability'].apply(risk_zone_function)

print(new_company[['tic', 'fyear', 'lda_probability', 'risk_zone']])

# Step 1: Get all used company tickers from your sampled_non_bankrupt_df
used_companies = sampled_non_bankrupt_df['tic'].unique()

# Step 2: Filter companies not used in training (non-bankrupt unseen)
unseen_companies_df = healthcare_non_bankrupt[~healthcare_non_bankrupt['tic'].isin(used_companies)]

# Step 3: Select only 2024 data
unseen_2024_df = unseen_companies_df[unseen_companies_df['fyear'] == 2024]

# âœ… Optional: Drop NaNs or fix them
unseen_2024_df = unseen_2024_df.dropna(subset=['X1', 'X2', 'X3', 'X4', 'X5'])

# Step 4: Extract features
new_X = unseen_2024_df[['X1', 'X2', 'X3', 'X4', 'X5']]

# Step 5: Handle infs and NaNs just in case
new_X.replace([np.inf, -np.inf], np.nan, inplace=True)
new_X.fillna(X.mean(), inplace=True)  # X is your training feature data

# Step 6: Scale features using trained scaler
new_X_scaled = scaler.transform(new_X)

# Step 7: Predict probability
unseen_2024_df['lda_probability'] = lda.predict_proba(new_X_scaled)[:, 1]

# Step 8: Label zones based on your custom function
unseen_2024_df['risk_zone'] = unseen_2024_df['lda_probability'].apply(risk_zone_function)

# Step 9: Show results
print(unseen_2024_df[['tic', 'fyear', 'lda_probability', 'risk_zone']])

# 'pfe_in_sampled' in sampled_non_bankrupt_df['tic'].unique()
# # OR, more explicitly:
"JNJ" in sampled_non_bankrupt_df['tic'].unique()

# Step 1: Pull JNJ's 2024 data from the non-bankrupt pool
jnj_2024 = healthcare_non_bankrupt[
    (healthcare_non_bankrupt['tic'] == 'JNJ') &
    (healthcare_non_bankrupt['fyear'] == 2024)
].copy()

# Step 2: Ensure no NaNs in financial ratios
jnj_2024 = jnj_2024.dropna(subset=['X1', 'X2', 'X3', 'X4', 'X5'])

# Step 3: Feature extraction + cleaning
jnj_X = jnj_2024[['X1', 'X2', 'X3', 'X4', 'X5']]
jnj_X.replace([np.inf, -np.inf], np.nan, inplace=True)
jnj_X.fillna(X.mean(), inplace=True)

# Step 4: Scale using the same scaler from training
jnj_X_scaled = scaler.transform(jnj_X)

# Step 5: Predict LDA probability
jnj_2024['lda_probability'] = lda.predict_proba(jnj_X_scaled)[:, 1]

# Step 6: Assign risk zone
jnj_2024['risk_zone'] = jnj_2024['lda_probability'].apply(risk_zone_function)

# Step 7: Show result
print(jnj_2024[['tic', 'fyear', 'lda_probability', 'risk_zone']])